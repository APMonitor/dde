{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Analysis with Python\n",
    "\n",
    "[Audio Data with Python](https://www.apmonitor.com/dde/index.php/Main/AudioAnalysis) in the [Data-Driven Engineering](http://apmonitor.com/dde) online course.\n",
    "\n",
    "<img align=left width=500px src='https://apmonitor.com/dde/uploads/Main/python_audio.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries for Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "f = 'Drone1.wav' # or Drone2.wav, Drone3.wav\n",
    "url = 'http://apmonitor.com/dde/uploads/Main/'+f\n",
    "urllib.request.urlretrieve(url,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the .wav audio\n",
    "# s = sampling (int)\n",
    "# a = audio signal (numpy array)\n",
    "s,a = wavfile.read(f)\n",
    "print('Sampling Rate:',s)\n",
    "print('Audio Shape:',np.shape(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Original Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "na = a.shape[0]\n",
    "# audio time duration\n",
    "la = na / s\n",
    "\n",
    "# plot signal versus time\n",
    "t = np.linspace(0,la, na)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(t,a[:,0],'b-')\n",
    "plt.ylabel('Left')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(t,a[:,1],'r-')\n",
    "plt.ylabel('Right')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to One Channel (Stereo to Mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound = AudioSegment.from_wav(f)\n",
    "sound = sound.set_channels(1)\n",
    "fm = f[:-4]+'_mono.wav'\n",
    "sound.export(fm,format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a = wavfile.read(fm)\n",
    "print('Sampling Rate:',s)\n",
    "print('Audio Shape:',np.shape(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Modified Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = a.shape[0]\n",
    "la = na / s\n",
    "t = np.linspace(0,la,na)\n",
    "plt.plot(t,a,'k-',color='purple')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze entire audio clip\n",
    "na = len(a)\n",
    "a_k = np.fft.fft(a)[0:int(na/2)]/na # FFT function from numpy\n",
    "a_k[1:] = 2*a_k[1:] # single-sided spectrum only\n",
    "Pxx = np.abs(a_k)   # remove imaginary part\n",
    "f = s*np.arange((na/2))/na # frequency vector\n",
    "\n",
    "# plotting\n",
    "fig,ax = plt.subplots()\n",
    "plt.plot(f,Pxx,'b-',label='Audio Signal')\n",
    "plt.plot([20,20000],[0.1,0.1],'r-',alpha=0.7,\\\n",
    "         linewidth=10,label='Audible (Humans)')\n",
    "ax.set_xscale('log'); ax.set_yscale('log')\n",
    "plt.grid(); plt.legend()\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first second clip\n",
    "na = s\n",
    "a_k = np.fft.fft(a[:na])[0:int(na/2)]/na # FFT function from numpy\n",
    "a_k[1:] = 2*a_k[1:] # single-sided spectrum only\n",
    "Pxx = np.abs(a_k)   # remove imaginary part\n",
    "f = s*np.arange((na/2))/na # frequency vector\n",
    "\n",
    "# plotting\n",
    "fig,ax = plt.subplots()\n",
    "plt.plot(f,Pxx,linewidth=1)\n",
    "ax.set_xscale('log'); ax.set_yscale('log')\n",
    "plt.ylabel('Amplitude'); plt.grid()\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr, tm, spgram = signal.spectrogram(a,s)\n",
    "lspg = np.log(spgram)\n",
    "plt.pcolormesh(tm,fr,lspg,shading='auto')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bin the Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the frequencies for machine learning features\n",
    "fb = np.array([0,10,20,30,50,75,100,150,200,400,600,\\\n",
    "               800,1000,1500,2000,2500,5000,20000,100000])\n",
    "Pb = np.zeros(len(fb))\n",
    "nb = np.zeros(len(fb))\n",
    "ibin = 0\n",
    "n = 0\n",
    "for i in range(len(f)):\n",
    "    if f[i]>fb[ibin+1]:\n",
    "        ibin+=1\n",
    "    nb[ibin]+=1\n",
    "    Pb[ibin]+=Pxx[i]\n",
    "for i in range(len(fb)):\n",
    "    if nb[i] == 0:\n",
    "        nb[i]=1\n",
    "    Pb[i] = Pb[i]/nb[i]\n",
    "fig,ax = plt.subplots()\n",
    "plt.semilogx(fb,Pb,'ro',linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze each sec of audio clip\n",
    "nsec = int(np.floor(la))\n",
    "pf = np.empty(nsec)\n",
    "for i in range(nsec):\n",
    "    audio = a[i*s:(i+1)*s]; na=len(audio) # use 48000 points with 48kHz\n",
    "    a_k = np.fft.fft(audio)[0:int(na/2)]/na\n",
    "    a_k[1:] = 2*a_k[1:] \n",
    "    Pxx = np.abs(a_k)   \n",
    "    f = s*np.arange((na/2))/na\n",
    "    ipf = np.argmax(Pxx)\n",
    "    pf[i] = f[ipf]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.pcolormesh(tm,fr,lspg,shading='auto')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.subplot(2,1,2)\n",
    "tb = np.arange(0,nsec)\n",
    "plt.bar(tb,pf)\n",
    "plt.xlabel('Time (sec)'); plt.ylabel('Peak Freq (Hz)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

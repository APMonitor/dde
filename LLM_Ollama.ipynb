{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM with Ollama Python Library\n",
    "\n",
    "[LLM with Ollama Python Library](https://apmonitor.com/dde/index.php/Main/LargeLanguageModel) in the [Data-Driven Engineering](http://apmonitor.com/dde) online course.\n",
    "\n",
    "<img align=left width=500px src='https://apmonitor.com/dde/uploads/Main/ollama_llm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ollama and transformers libraries are two packages that integrate Large Language Models (LLMs) with Python to provide chatbot and text generation capabilities. This tutorial covers the installation and basic usage of the ollama library.\n",
    "\n",
    "The first step is to install the [ollama server](https://ollama.com). After the server is running, install the [ollama python package](https://pypi.org/project/ollama/) with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ollama server and python package installed, retrieve the mistral LLM or any of the available [LLM models in the ollama library](https://ollama.com/library). The mistral model is a relatively small (7B parameter) LLM that can run on most CPUs. Larger models such as the mixtral model work best on GPUs with sufficient processing power and VRAM memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "ollama.pull('mistral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ollama and LLM model installation occurs once. List the available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model installed, use the generate function to send a prompt to the LLM and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'How can LLMs be used in engineering?'\n",
    "ollama.generate(model='mistral', prompt=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat function retains memory of prior prompts while the generate function does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "prompt1 = 'What is the capital of France?'\n",
    "response = ollama.chat(model='mistral', messages=[\n",
    "            {'role': 'user','content': prompt1,},])\n",
    "r1 = response['message']['content']\n",
    "print(r1)\n",
    "\n",
    "prompt2 = 'and of Germany?'\n",
    "response = ollama.chat(model='mistral', messages=[\n",
    "            {'role': 'user','content': prompt1,},\n",
    "            {'role': 'assistant','content': r1,},            \n",
    "            {'role': 'user','content': prompt2,},])\n",
    "r2 = response['message']['content']\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ollama library also supports streaming responses so that the text appears in pieces as it is generated instead of printed once after completion. This improves the perceived responsiveness of the LLM, especially with limited computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "prompt = 'How can LLMs improve automation?'\n",
    "stream = ollama.chat(model='mistral',\n",
    "          messages=[{'role': 'user', 'content': prompt}],\n",
    "          stream=True,)\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library API is designed to access the ollama REST API with functions like chat, generate, list, show, create, copy, delete, pull, push, and embeddings.\n",
    "\n",
    "**Applications in Engineering**\n",
    "\n",
    "The ollama python library facilitates LLMs in applications such as chatbots, customer support agents, and content generation tools. Code generation, debugging, and cross-language programming support can be accelerated with LLMs if used effectively. The ollama library simplifies interaction with advanced LLM models enabling more sophisticated responses and capabilities.\n",
    "\n",
    "One potential obstacle to using more sophisticated models is the size of the LLM and speed of response without a high-end GPU. Cloud computing resources are a viable option for application deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity: Evaluate Model Performance**\n",
    "\n",
    "It is important to understand the trade-offs for quality, cost, and speed for different LLMs. There are [AI model websites](https://artificialanalysis.ai) that benchmark model performance and service providers. The purpose of this exercise is to compare model performance on your computer. Install 3 different models on your local ollama server. Test the speed of response versus the size of the model with the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# question\n",
    "q = '''How can data science techniques improve predictive maintenance \n",
    "of rotating equipment such as pumps and turbines?'''\n",
    "\n",
    "# context\n",
    "cntx = '''In engineering, particularly in industries such as manufacturing, \n",
    "transportation, and energy, machinery and equipment are crucial assets. \n",
    "These assets require regular maintenance to ensure optimal performance \n",
    "and to prevent unexpected breakdowns, which can be costly and disruptive. \n",
    "Traditional maintenance strategies often rely on scheduled maintenance \n",
    "routines or responding to equipment failures as they occur. However, \n",
    "with the advancement of data science techniques such as machine learning \n",
    "and predictive analytics, engineers can now predict when a machine is \n",
    "likely to fail or require maintenance. This approach, known as predictive \n",
    "maintenance, uses historical data, sensor data, and algorithms to identify \n",
    "patterns and predict potential issues before they happen. The implementation \n",
    "of these data science techniques in engineering maintenance can lead to \n",
    "more efficient use of resources, reduced downtime, and potentially \n",
    "significant cost savings.'''\n",
    "\n",
    "# models\n",
    "models = ['phi','mistral','gemma'] #,'mixtral']\n",
    "\n",
    "# store results \n",
    "r = []\n",
    "idx = []\n",
    "# prompt without context\n",
    "pmpt = f'Question: {q}'\n",
    "for i,mx in enumerate(models):\n",
    "    r.append(ollama.generate(model=mx, prompt=pmpt))\n",
    "    idx.append(mx)\n",
    "    print(f\"Model: {idx[-1]}, Time: {r[-1]['total_duration']/1e9}s\")\n",
    "\n",
    "# prompt with context\n",
    "pmpt = f'Context: {cntx} Question: {q}'\n",
    "for i,mx in enumerate(models):\n",
    "    r.append(ollama.generate(model=mx, prompt=pmpt))\n",
    "    idx.append(mx+'+ctx')\n",
    "    print(f\"Model: {idx[-1]}, Time: {r[-1]['total_duration']/1e9}s\")\n",
    "\n",
    "# put results in DataFrame\n",
    "rcols = ['total_duration','load_duration',\n",
    "         'prompt_eval_count','eval_count',\n",
    "         'prompt_eval_duration','eval_duration']\n",
    "data = {}\n",
    "for i,x in enumerate(rcols):\n",
    "    if (i==2) or (i==3):\n",
    "        data[x] = [ri[x] for ri in r]\n",
    "    else:\n",
    "        data[x] = [ri[x]/1e9 for ri in r]\n",
    "data = pd.DataFrame(data, index=idx)\n",
    "data['calc_duration'] = data['total_duration']-data['load_duration']\n",
    "data['prompt_rate'] = data['prompt_eval_count']/data['prompt_eval_duration']\n",
    "data['eval_rate'] = data['eval_count']/data['eval_duration']\n",
    "print(data)\n",
    "\n",
    "pcols = ['load_duration','prompt_rate','eval_rate']\n",
    "axs = data[pcols].plot(figsize=(6,5),kind='bar',subplots=True)\n",
    "ylb = ['Time (s)','Rate (tk/s)','Rate (tk/s)']\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.set_title('')\n",
    "    ax.set_ylabel(ylb[i])\n",
    "plt.tight_layout(); plt.savefig('results.png',dpi=300); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

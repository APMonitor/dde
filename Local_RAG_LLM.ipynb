{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local RAG and LLM Integration\n",
    "\n",
    "[RAG and LLM Integration](https://apmonitor.com/dde/index.php/Main/RAGLargeLanguageModel) in the [Data-Driven Engineering](http://apmonitor.com/dde) online course.\n",
    "\n",
    "<img align=left width=500px src='https://apmonitor.com/dde/uploads/Main/RAG_LLM_NLP.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) leads to context-aware systems. RAG optimizes the output of a large language model by referencing an external authoritative knowledge base outside of initial training data sources. These external references generate a response to provide more accurate, contextually relevant, and up-to-date information. In this architecture, the LLM is the reasoning engine while the RAG context provides relevant data. This is different than fine-tuning where the LLM parameters are augmented based on a specific knowledge database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design function to create the ChromaDB vector store by reading and encoding [train.jsonl from GitHub](https://github.com/BYU-PRISM/GEKKO/blob/master/docs/llm/train.jsonl). More information on this topic is available in [RAG Similarity Search](https://apmonitor.com/dde/index.php/Main/SimilaritySearch).\n",
    "\n",
    "<img align=left width=300px src='https://apmonitor.com/dde/uploads/Main/similarity_search.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preparing the ChromaDB with data\n",
    "def setup_chromadb():\n",
    "    # read Gekko LLM training data\n",
    "    url='https://raw.githubusercontent.com'\n",
    "    path='/BYU-PRISM/GEKKO/master/docs/llm/train.jsonl'\n",
    "    qa = pd.read_json(url+path,lines=True)\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    for i in range(len(qa)):\n",
    "        s = f\"### Question: {qa['question'].iloc[i]} ### Answer: {qa['answer'].iloc[i]}\"\n",
    "        documents.append(s)\n",
    "        metadatas.append({'qid': f'qid_{i}'})\n",
    "        ids.append(str(i))\n",
    "\n",
    "    cc = chromadb.Client()\n",
    "    cdb = cc.create_collection(name='gekko')\n",
    "    cdb.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "    return cdb\n",
    "\n",
    "# Setup ChromaDB\n",
    "cdb = setup_chromadb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train.jsonl file contains hundreds of questions and answers about Gekko. It is used to provide context for the Gekko Support Agent that assists with questions about modeling and optimization in Python. The train.jsonl file is added to lists required to build the vector store with documents with the text, metadatas with a unique ID name, and ids with a unique integer identifier.\n",
    "\n",
    "Create the `ollama_llm` function to generate a response to a question. This requires the [Ollama Server](https://ollama.com) with the `mistral` model that runs locally. More information on this topic is available in the [Tutorial on LLM with Ollama Python Library](https://apmonitor.com/dde/index.php/Main/LargeLanguageModel).\n",
    "\n",
    "<img align=left width=300px src='https://apmonitor.com/dde/uploads/Main/ollama_llm.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama LLM function\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the RAG chain function that retrieves related information from the ChromaDB vector database. Feed this information as context with the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG chain\n",
    "def rag_chain(question, cdb):\n",
    "    context = cdb.query(query_texts=[question],\n",
    "                        n_results=5, include=['documents'])\n",
    "    formatted_context = \"\\n\\n\".join(x for x in context['documents'][0])\n",
    "    formatted_context += \"\\n\\nYou are a professional and technical assistant trained to answer questions about Gekko, which is a high-performance Python package for optimization, simulation, machine learning, data-science, model predictive control, and parameter estimation. In addition, you can also help with answering questions about programming in Python, particularly in relation to the aforementioned topics. Your primary goal is to assist users in finding solutions and gaining knowledge in these areas.\"\n",
    "    result = ollama_llm(question, formatted_context)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a prompt for the local RAG LLM and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt for Local RAG LLM\n",
    "question = 'What are you trained to do?'\n",
    "out = rag_chain(question, cdb)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Gekko AI Assistant](https://apmonitor.com/docs) (cloud solution) to ask the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gekko import support\n",
    "assistant = support.agent()\n",
    "assistant.ask(\"What are you trained to do?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
